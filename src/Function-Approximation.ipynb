{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d99a971c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pucrs-automated-planning/fa-rl/blob/main/src/Function-Approximation.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Tutorial 03: Function Approximation\n",
    "\n",
    "#### Prof. Felipe Meneguzzi\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this practical, you will implement one particular type of function approximation for reinforcement learning: [Tile Coding](http://www.incompleteideas.net/book/RLbook2018.pdf#page=239). Using this encoding we will implement function approximation in the [Sarsa](https://en.wikipedia.org/wiki/State–action–reward–state–action) algorithm. Our implementation will be based on a simple [RLAgent](rl/agent.py) class provided by the instructor, and the simplest of the [Gym](https://github.com/openai/gym) environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "You will need to configure your environment following the instructions in the `README.md` file included in this repository.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5478e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  print(\"We are in Google colab, we need to clone the repo\")\n",
    "  !git clone https://github.com/pucrs-automated-planning/fa-rl.git\n",
    "  %cd fa-rl\n",
    "  %pip install -r requirements.txt\n",
    "  %cd src\n",
    "except:\n",
    "  print(\"Not in colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent interface\n",
    "\n",
    "In this notebook we will use a basic agent interface called [`RLAgent`](src/rl/agent.py). Open this file and become familiar with its key attributes and methods. \n",
    "\n",
    "The key attributes you will need for this notebook are the following:\n",
    "- `self.env` — a reference to the [environment from gym](https://github.com/openai/gym/blob/58ed658d9b15fd410c50d1fdb25a7cad9acb7fa4/gym/core.py#L8);\n",
    "- `self.episodes` — the number of episodes to train the agent when invoking [`RLAgent.learn()`](rl/agent.py#L125);\n",
    "- `self.alpha` — the *learning rate* to be used when computing the RL update;\n",
    "- `self.gamma` — the *discount factor* for the return computed during training; and\n",
    "- `self.q_table` — a dictionary of lists of floating point numbers indexed by the state and an action (e.g., `self.q_table[state][action]` should return the q-value of the `state`-`action` pair)\n",
    "- `self.actions` — an integer representing the size of the action space\n",
    "- `self.last_state` — the last state visited by the agent during learning\n",
    "- `self.last_action` — the last action chosen by the agent during learning\n",
    "- `self._random` — a reference to a Python [Random](https://docs.python.org/3/library/random.html#random.Random) object.\n",
    "\n",
    "Pay attention to the `self.random` object, and use it instead of the unbound methods from Python's [random library](https://docs.python.org/3/library/random.html), otherwise the tests will not work. \n",
    "\n",
    "The key methods you should be aware of are the following:\n",
    "- [`policy`](rl/agent.py#L90) — returns the greedy policy following the currently learned q-values;\n",
    "- [`epsilon_greedy_policy`](rl/agent.py#L101) — returns an $\\epsilon$-greedy policy;\n",
    "- [`agent_start`](rl/agent.py#L55) — the method to be called once *before each episode*;\n",
    "- [`agent_step`](rl/agent.py#L67) — the method to be called once at *each time step* within an episode;\n",
    "- [`agent_end`](rl/agent.py#L80) — the method to be called once at the *end of each episode* when the agent reaches a *terminal state*; and\n",
    "- [`learn`](rl/agent.py#L125) — runs the underlying learning algorithm for `self.episodes` episodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Environment\n",
    "\n",
    "In this tutorial, we will use the *Mountain Car Task* introduced in [Section 10.1 of the textbook](http://www.incompleteideas.net/book/RLbook2018.pdf#page=267). \n",
    "\n",
    "The task is for an under powered car to make it to the top of a hill:\n",
    "\n",
    "![Mountain Car](img/mountain-car.svg \"Mountain Car\")\n",
    "\n",
    "The car is under-powered, so the agent needs to learn to rock back and forth to get enough momentum to reach the goal. At each time step the agent receives from the environment its current velocity (a float between -0.07 and 0.07), and its current position (a float between -1.2 and 0.5). Since our state is continuous there are a potentially infinite number of states that our agent could be in. We need a function approximation method to help the agent deal with this. In this notebook we will use tile coding. We provide a tile coding implementation for you to use, imported above with tiles3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from rl.agent import RLAgent, Action, State\n",
    "from random import Random\n",
    "from gym import Env\n",
    "from typing import Any, Collection, List, NoReturn, Sequence, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "import rl.tiles3 as tc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f400478e",
   "metadata": {},
   "source": [
    "#### Tile Coding Helper Function\n",
    "\n",
    "To begin we are going to build a tile coding class for our Sarsa agent that will make it easier to make calls to our tile coder. This is based on code we adapt from Rich Sutton. \n",
    "\n",
    "The text book introduces Tile coding in [Section 9.5.4 of the textbook](http://www.incompleteideas.net/book/RLbook2018.pdf#page=239) as a way to create features that can both provide good generalization and discrimination. It consists of multiple overlapping tilings, where each tiling is a partitioning of the space into tiles.\n",
    "\n",
    "![Tile Coding](img/tile-coding-example.svg \"Tile Coding\")\n",
    "\n",
    "To help keep our agent code clean we are going to make a function specific for tile coding for our Mountain Car environment. To help we are going to use the Tiles3 library. This is a Python 3 implementation of the tile coder. To start take a look at the documentation: [Tiles3 documentation](http://incompleteideas.net/tiles/tiles3.html)\n",
    "To get the tile coder working we need to implement a few pieces:\n",
    "- First: create an index hash table — this is done for you in the ```init``` function using ```tc.IHT```.\n",
    "- Second is to scale the inputs for the tile coder based on the number of tiles and the range of values each input could take. The tile coder needs to take in a number in range [0, 1], or scaled to be [0, 1] * num_tiles. For more on this refer to the [Tiles3 documentation](http://incompleteideas.net/tiles/tiles3.html).\n",
    "- Finally we call ```tc.tiles``` to get the active tiles back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41610788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarTileCoder:\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        \"\"\"\n",
    "        Initializes the MountainCar Tile Coder\n",
    "        Initializers:\n",
    "        iht_size -- int, the size of the index hash table, typically a power of 2\n",
    "        num_tilings -- int, the number of tilings\n",
    "        num_tiles -- int, the number of tiles. Here both the width and height of the\n",
    "                     tile coder are the same\n",
    "        Class Variables:\n",
    "        self.iht -- tc.IHT, the index hash table that the tile coder will use\n",
    "        self.num_tilings -- int, the number of tilings the tile coder will use\n",
    "        self.num_tiles -- int, the number of tiles the tile coder will use\n",
    "        \"\"\"\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, position: float, velocity: float) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Takes in a position and velocity from the mountaincar environment\n",
    "        and returns a numpy array of active tiles.\n",
    "        \n",
    "        Arguments:\n",
    "        position -- float, the position of the agent between -1.2 and 0.5\n",
    "        velocity -- float, the velocity of the agent between -0.07 and 0.07\n",
    "        returns:\n",
    "        tiles - np.array, active tiles\n",
    "        \"\"\"\n",
    "        # Use the ranges above and self.num_tiles to scale position and velocity to the range [0, 1]\n",
    "        # then multiply that range with self.num_tiles so it scales from [0, num_tiles]\n",
    "        \n",
    "        position_scaled = 0\n",
    "        velocity_scaled = 0\n",
    "        \n",
    "        # YOUR CODE HERE (2 lines)\n",
    "        position_scaled = ((position+1.2)/1.7)*self.num_tiles\n",
    "        velocity_scaled = ((velocity+0.07)/0.14)*self.num_tiles\n",
    "        # END CODE\n",
    "        \n",
    "        # get the tiles using tc.tiles, with self.iht, self.num_tilings and [scaled position, scaled velocity]\n",
    "        # nothing to implment here\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [position_scaled, velocity_scaled])\n",
    "        \n",
    "        return np.array(tiles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08860be3",
   "metadata": {},
   "source": [
    "We can now test whether this implementation worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a range of positions and velocities to test\n",
    "# then test every element in the cross-product between these lists\n",
    "pos_tests = np.linspace(-1.2, 0.5, num=5)\n",
    "vel_tests = np.linspace(-0.07, 0.07, num=5)\n",
    "tests = list(itertools.product(pos_tests, vel_tests))\n",
    "\n",
    "mctc = MountainCarTileCoder(iht_size=1024, num_tilings=8, num_tiles=2)\n",
    "\n",
    "t = []\n",
    "for test in tests:\n",
    "    position, velocity = test\n",
    "    tiles = mctc.get_tiles(position=position, velocity=velocity)\n",
    "    t.append(tiles)\n",
    "\n",
    "expected = [\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "    [0, 1, 8, 3, 9, 10, 6, 11],\n",
    "    [12, 13, 8, 14, 9, 10, 15, 11],\n",
    "    [12, 13, 16, 14, 17, 18, 15, 19],\n",
    "    [20, 21, 16, 22, 17, 18, 23, 19],\n",
    "    [0, 1, 2, 3, 24, 25, 26, 27],\n",
    "    [0, 1, 8, 3, 28, 29, 26, 30],\n",
    "    [12, 13, 8, 14, 28, 29, 31, 30],\n",
    "    [12, 13, 16, 14, 32, 33, 31, 34],\n",
    "    [20, 21, 16, 22, 32, 33, 35, 34],\n",
    "    [36, 37, 38, 39, 24, 25, 26, 27],\n",
    "    [36, 37, 40, 39, 28, 29, 26, 30],\n",
    "    [41, 42, 40, 43, 28, 29, 31, 30],\n",
    "    [41, 42, 44, 43, 32, 33, 31, 34],\n",
    "    [45, 46, 44, 47, 32, 33, 35, 34],\n",
    "    [36, 37, 38, 39, 48, 49, 50, 51],\n",
    "    [36, 37, 40, 39, 52, 53, 50, 54],\n",
    "    [41, 42, 40, 43, 52, 53, 55, 54],\n",
    "    [41, 42, 44, 43, 56, 57, 55, 58],\n",
    "    [45, 46, 44, 47, 56, 57, 59, 58],\n",
    "    [60, 61, 62, 63, 48, 49, 50, 51],\n",
    "    [60, 61, 64, 63, 52, 53, 50, 54],\n",
    "    [65, 66, 64, 67, 52, 53, 55, 54],\n",
    "    [65, 66, 68, 67, 56, 57, 55, 58],\n",
    "    [69, 70, 68, 71, 56, 57, 59, 58],\n",
    "]\n",
    "assert np.all(expected == np.array(t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning with Function Approximation\n",
    "\n",
    "**Description** \n",
    "\n",
    "As we saw in the lecture, Q-Learning with Function approximation replaces the update on specific states for the update on the model weights. Tile coding lets us do that without the gradients, since we are only updating the affected tiles. So, instead of applying the semi-gradient update:\n",
    "\n",
    "$$\\bm{w} \\gets \\bm{w} + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} \\hat{q}(S_{t+1}, a', \\bm{w}) - \\hat{q}(S_{t}, A_{t}, \\bm{w}) \\right]\\nabla\\hat{q}(S_{t}, A_{t}, \\bm{w})$$\n",
    "\n",
    "We can simply ignore the gradients:\n",
    "\n",
    "$$\\bm{w} \\gets \\bm{w} + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} \\hat{q}(S_{t+1}, a', \\bm{w}) - \\hat{q}(S_{t}, A_{t}, \\bm{w}) \\right]$$\n",
    "\n",
    "In the cell below, you should implement the following methods (as noted in the source code):\n",
    "- `best_action` - You should iterate over possible actions and compute the value using the weights and the unpacked features\n",
    "- `policy` and `epsilon_greedy_policy` - you can accomplish this by referring to the `best_action` method you just implemented\n",
    "- `agent_start` - initialize the last state with the initial state given by the environment and choose one initial action\n",
    "- `agent_step` - this should follow the algorithm in Section 9.3 from Sutton and Barto\n",
    "- `agent_end` - one last update at the end of the episode, but using just the old value of Q as the TD-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAQLearner(RLAgent):\n",
    "    \"\"\"\n",
    "    A simple tile-coded FA Q-Learning agent.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env: Env,\n",
    "                 episodes: int = 500,\n",
    "                 decaying_eps: bool = True,\n",
    "                 eps: float = 1.0,\n",
    "                 alpha: float = 0.5,\n",
    "                 decay: float = 0.000002,\n",
    "                 gamma: float = 1.0,\n",
    "                 rand: Random = Random(),\n",
    "                 iht_size: int = 4096,\n",
    "                 w: ArrayLike = None,\n",
    "                 num_tilings: int = 8,\n",
    "                 num_tiles: int = 8,\n",
    "                 initial_weights: float = 0.0,\n",
    "                #  num_actions: int = 3,\n",
    "                 **kwargs):\n",
    "        super().__init__(env, episodes=episodes, decaying_eps=decaying_eps, eps=eps, alpha=alpha, decay=decay, gamma=gamma, rand=rand)\n",
    "        # self.actions = env.action_space.n\n",
    "        self.actions = 3\n",
    "        # Function approximation parameters\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.iht_size = iht_size\n",
    "        self.initial_weights = initial_weights\n",
    "        # self.num_actions = num_actions # TODO Double check that this is not redundant with self.actions\n",
    "\n",
    "        # We initialize self.mctc to the mountaincar verions of the \n",
    "        # tile coder that we created\n",
    "        self.tc = MountainCarTileCoder(iht_size=self.iht_size, \n",
    "                                         num_tilings=self.num_tilings, \n",
    "                                         num_tiles=self.num_tiles)\n",
    "\n",
    "        # We initialize self.w to three times the iht_size. Recall this is because\n",
    "        # we need to have one set of weights for each action.\n",
    "        self.w = np.ones((self.actions, self.iht_size)) * self.initial_weights\n",
    "\n",
    "        # hyperparameters\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        self.c_eps = eps\n",
    "        self.base_eps = eps\n",
    "        if decaying_eps:\n",
    "            def epsilon():\n",
    "                self.c_eps = max((self.episodes - self.step)/self.episodes, 0.01)\n",
    "                return self.c_eps\n",
    "            self.eps = epsilon\n",
    "        else:\n",
    "            self.eps = lambda: eps\n",
    "        self.decaying_eps = decaying_eps\n",
    "        # self.alpha = alpha\n",
    "        self.alpha = alpha / self.num_tilings # To comply with the course from coursera\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return np.random.choice(ties)\n",
    "\n",
    "    def get_active_tiles(self, state: State) -> ArrayLike:\n",
    "        position, velocity = state\n",
    "        \n",
    "        # Use self.tc to set active_tiles using position and velocity\n",
    "        active_tiles = self.tc.get_tiles(position, velocity)\n",
    "        return active_tiles\n",
    "\n",
    "    def best_action(self, tiles: ArrayLike) -> Tuple[Action, float]:\n",
    "        \"\"\"Returns the best action for a state, breaking ties randomly\n",
    "\n",
    "        Args:\n",
    "            state (State): The state in which to extract the best action\n",
    "\n",
    "        Returns:\n",
    "            int: The index of the best action\n",
    "        \"\"\"\n",
    "        action_values = []\n",
    "        chosen_action = None\n",
    "\n",
    "        ## YOUR CODE HERE (3 lines)\n",
    "        \n",
    "        \n",
    "\n",
    "        ## END CODE\n",
    "    \n",
    "    def get_max_q(self, state: State) -> float:\n",
    "        tiles = self.get_active_tiles(state)\n",
    "        return np.max(self.best_action(tiles)[0])\n",
    "    \n",
    "    def get_q_value(self, state: State, action: Any) -> float:\n",
    "        tiles = self.get_active_tiles(state)\n",
    "        return np.sum(self.w[action][tiles])\n",
    "\n",
    "    \n",
    "    def policy(self, tiles: ArrayLike) -> Tuple[Action, float]:\n",
    "        \"\"\"Returns the greedy deterministic policy for the specified state\n",
    "\n",
    "        Args:\n",
    "            state (State): the state for which we want the action\n",
    "\n",
    "        Raises:\n",
    "            InvalidAction: Not sure about this one\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Action, float]: The greedy action learned for state and its value\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (1 line)\n",
    "        \n",
    "        ## END CODE\n",
    "\n",
    "    def epsilon_greedy_policy(self, tiles: ArrayLike) -> Tuple[Action, float]:\n",
    "        \"\"\"Returns the epsilon-greedy policy\n",
    "\n",
    "        Args:\n",
    "            state (State): The state for which to return the epsilon greedy policy\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Action, float]: The action to be taken and its value\n",
    "        \"\"\"\n",
    "        action_values = []\n",
    "        chosen_action = None\n",
    "        eps = self.eps()\n",
    "        ## YOUR CODE HERE (6 lines)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## END CODE\n",
    "        return chosen_action, action_values[chosen_action]\n",
    "    \n",
    "    def agent_start(self, state: State) -> Action:\n",
    "        \"\"\"The first method called when the experiment starts,\n",
    "        called after the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's env_start function.\n",
    "        Returns:\n",
    "            (int) the first action the agent takes.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (2 lines)\n",
    "        \n",
    "\n",
    "        ## END CODE\n",
    "        self.last_state = state\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward: float, state: State) -> Action:\n",
    "        \"\"\"A step taken by the agent.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Any): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            (int) The action the agent takes given this state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (6 lines)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## END CODE\n",
    "        \n",
    "        # action = self.best_action(state)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward: float) -> NoReturn:\n",
    "        \"\"\"Called when the agent terminates.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (3 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Q-Learning\n",
    "## Mini test\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "agent = FAQLearner(env, decaying_eps=False, eps=0.1, rand=Random(0))\n",
    "agent.w = np.array([np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])])\n",
    "\n",
    "action_distribution = np.zeros(3)\n",
    "for i in range(1000):\n",
    "    chosen_action, action_value = agent.epsilon_greedy_policy(np.array([0,1]))\n",
    "    action_distribution[chosen_action] += 1\n",
    "    \n",
    "print(\"action distribution:\", action_distribution)\n",
    "# notice that the two non-greedy actions are roughly uniformly distributed\n",
    "assert np.all(action_distribution == [37, 23, 940])\n",
    "\n",
    "agent = FAQLearner(env, decaying_eps=False, eps=0.0, rand=Random(0))\n",
    "agent.w = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "chosen_action, action_value = agent.epsilon_greedy_policy(np.array([0,1]))\n",
    "assert chosen_action == 2\n",
    "assert action_value == 15\n",
    "\n",
    "# test update\n",
    "\n",
    "agent = FAQLearner(env, decaying_eps=False, eps=0.1, rand=Random(0))\n",
    "\n",
    "agent.agent_start((0.1, 0.3))\n",
    "agent.agent_step(1, (0.02, 0.1))\n",
    "\n",
    "assert np.all(agent.w[0,0:8] == 0.0625)\n",
    "assert np.all(agent.w[1:] == 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad57f71f",
   "metadata": {},
   "source": [
    "Now let us test our agent in the Mountain Car environment from Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "num_episodes = 50\n",
    "all_steps = []\n",
    "all_rewards = []\n",
    "\n",
    "agent = FAQLearner\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.seed(0)\n",
    "env._max_episode_steps = 15000\n",
    "start = time.time()\n",
    "\n",
    "# q_agent = FAQLearner(env, episodes=num_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "# rewards_q, steps_per_episode = q_agent.learn(max_tsteps=15000)\n",
    "\n",
    "for run in range(num_runs):\n",
    "    q_agent = agent(env, episodes=num_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "    \n",
    "    if run % 5 == 0:\n",
    "        print(\"RUN: {}\".format(run))\n",
    "    rewards_per_episode, steps_per_episode = q_agent.learn(max_tsteps=15000)\n",
    "\n",
    "    all_rewards.append(rewards_per_episode)\n",
    "    all_steps.append(steps_per_episode)\n",
    "\n",
    "print(\"Run time: {}\".format(time.time() - start))\n",
    "\n",
    "mean_steps = np.mean(all_steps, axis=0)\n",
    "plt.title(\"Mean steps\")\n",
    "plt.plot(mean_steps, label='Q-Learning')\n",
    "plt.show()\n",
    "print(\"Mean Steps\", mean_steps)\n",
    "plt.title(\"Mean Reward\")\n",
    "mean_reward = np.mean(all_rewards, axis=0)\n",
    "plt.plot(mean_reward, label='Q-Learning')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa\n",
    "\n",
    "Sarsa is an on policy algorithm where the source and target policy are the same. In our implementation, we will use the $\\epsilon$-greedy policy we implemented above.\n",
    "\n",
    "$$\\bm{w} \\gets \\bm{w} + \\alpha \\left[ R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\bm{w}) - \\hat{q}(S_{t}, A_{t}, \\bm{w}) \\right]$$\n",
    "\n",
    "Note that we can implement Sarsa by simply subclassing the `FAQLearner` class we just implemented. This time, the only modification we need is in the `agent_step` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FASarsaLearner(FAQLearner):\n",
    "    def agent_step(self, reward: float, state: State) -> Action:\n",
    "        \"\"\"A step taken by the agent.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Any): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            (int) The action the agent takes given this state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (5 lines)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        ## END CODE\n",
    "\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Sarsa\n",
    "num_runs = 10\n",
    "num_episodes = 50\n",
    "all_steps = []\n",
    "all_rewards = []\n",
    "\n",
    "agent = FASarsaLearner\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.seed(0)\n",
    "env._max_episode_steps = 15000\n",
    "start = time.time()\n",
    "\n",
    "for run in range(num_runs):\n",
    "    sarsa_agent = agent(env, episodes=num_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "    \n",
    "    if run % 5 == 0:\n",
    "        print(\"RUN: {}\".format(run))\n",
    "    rewards_per_episode, steps_per_episode = sarsa_agent.learn(max_tsteps=15000)\n",
    "\n",
    "    all_rewards.append(rewards_per_episode)\n",
    "    all_steps.append(steps_per_episode)\n",
    "\n",
    "print(\"Run time: {}\".format(time.time() - start))\n",
    "\n",
    "mean_steps = np.mean(all_steps, axis=0)\n",
    "plt.title(\"Mean steps\")\n",
    "plt.plot(mean_steps, label='Sarsa')\n",
    "plt.show()\n",
    "print(\"Mean Steps\", mean_steps)\n",
    "plt.title(\"Mean Reward\")\n",
    "mean_reward = np.mean(all_rewards, axis=0)\n",
    "plt.plot(mean_reward, label='Sarsa')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Learning Methods\n",
    "\n",
    "Our final experiment compares the accumulated rewards for each of our TD learning algorithms. \n",
    "<!-- This experiment replicates exactly example 6.6 (plus Expected Sarsa) in the book by plotting the cumulative rewards per time step of training each of our algorithms achieves in the Cliff Walking environment.  -->\n",
    "\n",
    "<!-- If your implementation is correct, you should see a graph like the one below:\n",
    "![\"Cumulative Rewards\"](figure_example_6_6.svg). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "num_episodes = 50\n",
    "q_steps = []\n",
    "q_rewards = []\n",
    "sarsa_steps = []\n",
    "sarsa_rewards = []\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.seed(0)\n",
    "env._max_episode_steps = 15000\n",
    "start = time.time()\n",
    "\n",
    "# q_agent = FAQLearner(env, episodes=num_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "# rewards_q, steps_per_episode = q_agent.learn(max_tsteps=15000)\n",
    "\n",
    "for run in range(num_runs):\n",
    "    q_agent = FAQLearner(env, episodes=num_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "    sarsa_agent = FASarsaLearner(env, episodes=num_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "    \n",
    "    if run % 5 == 0:\n",
    "        print(\"RUN: {}\".format(run))\n",
    "    rewards_per_episode, steps_per_episode = q_agent.learn(max_tsteps=15000)\n",
    "    q_rewards.append(rewards_per_episode)\n",
    "    q_steps.append(steps_per_episode)\n",
    "\n",
    "    rewards_per_episode, steps_per_episode = sarsa_agent.learn(max_tsteps=15000)\n",
    "    sarsa_rewards.append(rewards_per_episode)\n",
    "    sarsa_steps.append(steps_per_episode)\n",
    "\n",
    "print(\"Run time: {}\".format(time.time() - start))\n",
    "\n",
    "plt.title(\"Mean steps\")\n",
    "mean_steps = np.mean(q_steps, axis=0)\n",
    "plt.plot(mean_steps, label='Q-Learning')\n",
    "mean_steps = np.mean(sarsa_steps, axis=0)\n",
    "plt.plot(mean_steps, label='Sarsa')\n",
    "plt.show()\n",
    "# print(\"Mean Steps\", mean_steps)\n",
    "plt.title(\"Mean Reward\")\n",
    "mean_reward = np.mean(q_rewards, axis=0)\n",
    "plt.plot(mean_reward, label='Q-Learning')\n",
    "mean_reward = np.mean(sarsa_rewards, axis=0)\n",
    "plt.plot(mean_reward, label='Sarsa')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e181e91a4207cd28265f13374a7fae83cda3ae570825d0fda8366d350b98fe83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
